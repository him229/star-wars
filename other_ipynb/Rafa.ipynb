{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and Motivation\n",
    "\n",
    "Contributions in open source have really moved forward with platforms like Github and bitbucket. This recent trend has allowed for large databases that run software programs to be published openly, and also allows for people from all over the world to contribute to the code base. With Open Source being so prevalent in the technology culture, we decided to analyze some of the most popular Github profiles to find trends that might help provoke what makes these projects so popular, and also find trends in technologies that software engineers are using these days.\n",
    "\n",
    "# Related Work\n",
    "\n",
    "We did not find any related work.\n",
    "\n",
    "# Initial Questions\n",
    "\n",
    "Aside from asking the initial question of \"What trends in technology can we find from Github repos\" and \"What makes them popular\" we wanted to see if we could come up with a classifier that would dictate whether or not a respoitory with a few commits would become a popular open source project. We strayed away from building the classifier, as we didn't feel like we could collect sufficient data to get a good enough ballpark prediction, and because of common feedback loops such as companies open sourcing some of there code base and already having a cult like following for it.\n",
    "\n",
    "Some of the questions we wanted to look into were:\n",
    "  1. What is are the most common languages used?\n",
    "  2. How many commits do the most starred repositories have?\n",
    "  3. How many contributors?\n",
    "  4. Is there any correlation between commits and contributors\n",
    "  \n",
    "# Data Collection\n",
    "\n",
    "We tried using the Github API, but we got rate limited and were only allowed to hit the API endpoint 5,000 times an hour. This grew tedious so instead we used Beautiful Soup to scrape the data off of Github, and we set up a Twilio server that would text us every time we needed to restart the scrape. The data we collected was \n",
    "\n",
    "  - branches_count\n",
    "  - commits_count\n",
    "  - contributors_count\n",
    "  - forks_count\n",
    "  - has_downloads\n",
    "  - has_homepage\n",
    "  - has_issues\n",
    "  - has_pages\n",
    "  - has_wiki\n",
    "  - id\n",
    "  - milestones_count\n",
    "  - open_issues_count\n",
    "  - projects_count\n",
    "  - pull_requests_count\n",
    "  - release_count\n",
    "  - size\n",
    "  - star_count\n",
    "  - watchers count\n",
    "  \n",
    "All of this data is for the top 900 most starred repositories on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs489]",
   "language": "python",
   "name": "conda-env-cs489-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
