{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_repo_data(target_stars, date_since, file_name):\n",
    "    \"\"\"\n",
    "    This block gets all the popular repo data from Github.\n",
    "    \"\"\"\n",
    "    json_content = {'data': []}\n",
    "    for i in xrange(1,11):\n",
    "        url = 'https://api.github.com/search/repositories?q=created:%3E{}+stars:%3E{}&sort=stars&order=desc&per_page=100&page={}'.format(date_since, target_stars, i)\n",
    "        res = requests.get(url)\n",
    "        json_content['data'] += res.json()['items']\n",
    "    #saves the data to a file in json format\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(json.dumps(json_content, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test get repo data function\n",
    "\"\"\"\n",
    "def get_repo_master_data():\n",
    "    \"\"\"\n",
    "    Defines what 'popular' for a repo means\n",
    "    \"\"\"\n",
    "    target_stars = 5000\n",
    "    date_since = '2000-11-06'\n",
    "    file_name = 'repo_master_data_json.txt' # File where repo info is stored\n",
    "    get_repo_data(target_stars, date_since, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT the line below to fetch info for popular repos\n",
    "\n",
    "# get_repo_master_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'repo_master_data_json.txt' # File where most popular repo info is stored\n",
    "\n",
    "def test_popular_repos_file(file_name, num):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print len(data['data'])\n",
    "    print json.dumps(data['data'][num], sort_keys=True, indent=4, separators=(',', ': '))\n",
    "\n",
    "# UNCOMMENT below to test any entry in the popular repos file    \n",
    "    \n",
    "# test_popular_repos_file(file_name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from configobj import ConfigObj\n",
    "\n",
    "\"\"\"\n",
    "Github API keys\n",
    "\"\"\"\n",
    "\n",
    "config = ConfigObj('config.ini')\n",
    "client_id = config['github_api_id']\n",
    "client_secret = config['github_api_secret']\n",
    "\n",
    "GITHUB_AUTH = '?client_id={}&client_secret={}'.format(client_id, client_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_repo_commit_data(file_name, repo_master_file_name):\n",
    "    \"\"\"\n",
    "    Gets commit data for each repo inside the popular repos file\n",
    "    \"\"\"\n",
    "    with open(repo_master_file_name, 'r') as f:\n",
    "        master_data = json.load(f) # reads file with most popular repos into a dict\n",
    "        commits_data = {}\n",
    "        for repo in master_data['data']:\n",
    "            # commits_url, name\n",
    "            url = repo['commits_url']\n",
    "            commits_url = url[:len(url)-6] + GITHUB_AUTH\n",
    "            res = requests.get(commits_url)\n",
    "            commits_data[repo['id']] = {'name': repo['name'], 'commits': res.json()}\n",
    "            print repo['name'], len(commits_data), res.status_code\n",
    "\n",
    "        # wirtes the commit_data dict to a file\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json.dumps(commits_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT the line below to fetch commit info for popular repos\n",
    "\n",
    "#get_repo_commit_data('repo_commit_data_json.txt', 'repo_master_data_json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "def test_count_commits_info_repos(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        commit_data = json.load(f)\n",
    "        print len(commit_data)\n",
    "        \n",
    "# UNCOMMENT below to test the count\n",
    "        \n",
    "# file_name = 'repo_commit_data_json.txt'\n",
    "# test_count_commits_info_repos(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "\n",
    "def get_repo_commit_data(file_name, repo_master_file_name):\n",
    "    \"\"\"\n",
    "    Get all the contributor information\n",
    "    \"\"\"\n",
    "    with open(repo_master_file_name, 'r') as f:\n",
    "        master_data = json.load(f)\n",
    "        contributor_data = {}\n",
    "        for repo in master_data['data']:\n",
    "            url = repo['contributors_url'] + GITHUB_AUTH\n",
    "            res = requests.get(url)\n",
    "            contributor_data[repo['id']] = {'name': repo['name'], 'contributors': res.json()}\n",
    "            print repo['id'], repo['name'], len(contributor_data), res.status_code\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json.dumps(contributor_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT the line below to fetch commit info for popular repos\n",
    "\n",
    "#get_repo_commit_data('repo_contributor_data_json.txt', 'repo_master_data_json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all the issue comments data\n",
    "\"\"\"\n",
    "with open('repo_master_data_json.txt', 'r') as f:\n",
    "    master_data = json.load(f)\n",
    "    issue_comment_data = {}\n",
    "    for repo in master_data['data']:\n",
    "        url = repo['issue_comment_url']\n",
    "        url = url[:len(url)-9] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "        res = requests.get(url)\n",
    "        comments = []\n",
    "        for comment in res.json():\n",
    "            comments.append(comment['body'])\n",
    "        issue_comment_data[repo['id']] = {'name': repo['name'], 'comments': comments}\n",
    "        print repo['id'], repo['name'], len(issue_comment_data), res.status_code\n",
    "    with open('repo_issue_comments_json.txt','w') as f:\n",
    "        f.write(json.dumps(issue_comment_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all the issues data\n",
    "\"\"\"\n",
    "with open('repo_master_data_json.txt', 'r') as f:\n",
    "    master_data = json.load(f)\n",
    "    issue_comment_data = {}\n",
    "    for repo in master_data['data']:\n",
    "        url = repo['issues_url']\n",
    "        url = url[:len(url)-9] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "        res = requests.get(url)\n",
    "        contents = []\n",
    "        for entry in res.json():\n",
    "            contents.append(entry['body'])\n",
    "        issue_comment_data[repo['id']] = {'name': repo['name'], 'contents': contents}\n",
    "        print repo['id'], repo['name'], len(issue_comment_data), res.status_code\n",
    "    with open('repo_issue_json.txt','w') as f:\n",
    "        f.write(json.dumps(issue_comment_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all the languages data\n",
    "\"\"\"\n",
    "with open('repo_master_data_json.txt', 'r') as f:\n",
    "    master_data = json.load(f)\n",
    "    issue_comment_data = {}\n",
    "    for repo in master_data['data']:\n",
    "        url = repo['languages_url']\n",
    "        url = url + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "        res = requests.get(url)\n",
    "        issue_comment_data[repo['id']] = {'name': repo['name'], 'languages': res.json()}\n",
    "        print repo['id'], repo['name'], len(issue_comment_data), res.status_code\n",
    "    with open('repo_languages_json.txt','w') as f:\n",
    "        f.write(json.dumps(issue_comment_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all the milestone data\n",
    "\"\"\"\n",
    "with open('repo_master_data_json.txt', 'r') as f:\n",
    "    master_data = json.load(f)\n",
    "    issue_comment_data = {}\n",
    "    for repo in master_data['data']:\n",
    "        url = repo['milestones_url']\n",
    "        url = url[:len(url)-9] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "        res = requests.get(url)\n",
    "        contents = []\n",
    "        for entry in res.json():\n",
    "            contents.append({'title': entry['title'], 'description': entry['description'], 'open_issues': entry['open_issues'], 'closed_issues': entry['closed_issues']})\n",
    "        issue_comment_data[repo['id']] = {'name': repo['name'], 'milestones': contents}\n",
    "        print repo['id'], repo['name'], len(issue_comment_data), res.status_code\n",
    "    with open('repo_milestones_json.txt','w') as f:\n",
    "        f.write(json.dumps(issue_comment_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get concrete contributors data\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "with open('repo_contributor_data_json.txt') as f, open('repo_master_data_json.txt') as master:\n",
    "    contributor_data = json.load(f)\n",
    "    master_data = json.load(master)['data'][:201]\n",
    "    concrete_contributor = defaultdict(list)\n",
    "    for repo in master_data:\n",
    "        if repo['name'] == 'linux': continue\n",
    "        contributors = contributor_data[str(repo['id'])]['contributors']\n",
    "        for individual in contributors[:5] if len(contributors) > 5 else contributors:\n",
    "            followers_url = individual['followers_url'] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "            #following_url = individual['following_url']\n",
    "            #following_url = following_url[:len(following_url)-13]\n",
    "            #gists_url = individual['gists_url']\n",
    "            #gists_url = gists_url[:len(gists_url)-10]\n",
    "            starred_url = individual['starred_url']\n",
    "            starred_url = starred_url[:len(starred_url)-15] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "            organizations_url = individual['organizations_url'] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "            repos_url = individual['repos_url'] + '?client_id={}&client_secret={}'.format(client_id, client_secret)\n",
    "            \n",
    "            followers_res = requests.get(followers_url)\n",
    "            followers_json = followers_res.json()\n",
    "            starred_res = requests.get(starred_url)\n",
    "            starred_json = starred_res.json()\n",
    "            organizations_res = requests.get(organizations_url)\n",
    "            organizations_json = organizations_res.json()\n",
    "            repos_res = requests.get(repos_url)\n",
    "            repos_json = repos_res.json()\n",
    "            print repo['name'], individual['id'], followers_res.status_code, starred_res.status_code, organizations_res.status_code, repos_res.status_code\n",
    "            concrete_contributor[repo['id']].append({'individual_id': individual['id'],\n",
    "                                                'login': individual['login'],\n",
    "                                                'followers': len(followers_json),\n",
    "                                               'starred': len(starred_json),\n",
    "                                               'repos': len(repos_json),\n",
    "                                               'organizations': len(organizations_json)})\n",
    "        print repo['id'], repo['name'], len(concrete_contributor)\n",
    "    with open('repo_concrete_contributors_json.txt','w') as ff:\n",
    "        ff.write(json.dumps(concrete_contributor, sort_keys=True, indent=4, separators=(',', ': ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def clean_up(s):\n",
    "    s = str(s)\n",
    "    if 'k' in s:\n",
    "        return int(float(s[:-1]) * 1000)\n",
    "    else:\n",
    "        return int(s)\n",
    "    \n",
    "with open('repo_concrete_contributors_json2.txt', 'r') as f:\n",
    "    concrete_contributor = json.load(f)\n",
    "    for values in concrete_contributor.values():\n",
    "        for value in values:\n",
    "            print value['login']\n",
    "            try:\n",
    "                html = requests.get('https://github.com/' + value['login'] + '/').content\n",
    "                soup = BeautifulSoup(html)\n",
    "                data = soup.find_all('span', {'class' : 'counter'}, text=True)\n",
    "                repositories = data[0].get_text().strip()\n",
    "                stars = data[1].get_text().strip()\n",
    "                followers = data[2].get_text().strip()\n",
    "                following = data[3].get_text().strip()\n",
    "                print repositories, stars, followers, following\n",
    "            except:\n",
    "                print \"^^^^^^^^error above name\"\n",
    "                continue\n",
    "            value['followers'] = clean_up(followers)\n",
    "            value['following'] = clean_up(following)\n",
    "            value['starred'] = clean_up(stars)\n",
    "            value['repos'] = clean_up(repositories)\n",
    "\n",
    "    with open('repo_concrete_contributors_json3.txt', 'w') as ff:\n",
    "        ff.write(json.dumps(concrete_contributor, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "def clean_up(s):\n",
    "    s = str(s)\n",
    "    if 'k' in s:\n",
    "        return int(float(s[:-1]) * 1000)\n",
    "    else:\n",
    "        return int(s)\n",
    "    \n",
    "with open('repo_contributor_data_json.txt', 'r') as f:\n",
    "    contributors = json.load(f)\n",
    "    concrete_contributors = defaultdict(list)\n",
    "    for key, values in contributors.iteritems():\n",
    "        for value in values['contributors']:\n",
    "            print value['login']\n",
    "            try:\n",
    "                html = requests.get('https://github.com/' + value['login'] + '/').content\n",
    "                soup = BeautifulSoup(html)\n",
    "                data = soup.find_all('span', {'class' : 'counter'}, text=True)\n",
    "                repositories = data[0].get_text().strip()\n",
    "                stars = data[1].get_text().strip()\n",
    "                followers = data[2].get_text().strip()\n",
    "                following = data[3].get_text().strip()\n",
    "                print repositories, stars, followers, following\n",
    "            except IndexError:\n",
    "                print \"^^^^^^^^error above name\"\n",
    "                continue\n",
    "            concrete_contributors[key].append({\n",
    "                    'login': value['login'],\n",
    "                    'id': value['id'],\n",
    "                    'followers': clean_up(followers),\n",
    "                    'following':clean_up(following),\n",
    "                    'starred':clean_up(stars),\n",
    "                    'repos':clean_up(repositories)\n",
    "                })\n",
    "    with open('repo_concrete_contributors_json.txt', 'w') as ff:\n",
    "        ff.write(json.dumps(concrete_contributors, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print concrete_contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "def clean_and_get_text_data(s):\n",
    "    try:\n",
    "        return int(s.get_text().strip().replace(',','').replace('+',''))\n",
    "    except UnicodeEncodeError:\n",
    "        return -1\n",
    "\n",
    "def scrape_repo_info(username, reponame):\n",
    "    print username, reponame\n",
    "#     username = 'fortawesome'\n",
    "#     reponame = 'font-awesome'\n",
    "    html = requests.get('https://github.com/' + username + '/' + reponame).content\n",
    "    soup = BeautifulSoup(html)\n",
    "    data_1 = soup.find_all('span', {'class' : 'num text-emphasized'}, text=True)\n",
    "    data_2 = soup.find_all('span', {'class' : 'counter'}, text=True)\n",
    "    \n",
    "    commits, branches, release, contributors, issues, pull_requests, projects = 0,0,0,0,0,0,0\n",
    "#     print data_1, data_2\n",
    "    if len(data_2) == 2:\n",
    "        commits = clean_and_get_text_data(data_1[0])\n",
    "        branches = clean_and_get_text_data(data_1[1])\n",
    "        release = clean_and_get_text_data(data_1[2])\n",
    "        contributors = clean_and_get_text_data(data_1[3])\n",
    "#         issues = clean_and_get_text_data(data_2[0])\n",
    "        pull_requests = clean_and_get_text_data(data_2[0])\n",
    "        projects = clean_and_get_text_data(data_2[1])\n",
    "    else:\n",
    "        #assert len(data_2) == 3\n",
    "        commits = clean_and_get_text_data(data_1[0])\n",
    "        branches = clean_and_get_text_data(data_1[1])\n",
    "        release = clean_and_get_text_data(data_1[2])\n",
    "        contributors = clean_and_get_text_data(data_1[3])\n",
    "        issues = clean_and_get_text_data(data_2[0])\n",
    "        pull_requests = clean_and_get_text_data(data_2[1])\n",
    "        projects = clean_and_get_text_data(data_2[2])\n",
    "\n",
    "    data = {'commits': commits,\n",
    "            'branches': branches,\n",
    "            'release': release,\n",
    "            'contributors': contributors,\n",
    "            'issues': issues,\n",
    "            'pull_requests': pull_requests,\n",
    "            'projects': projects\n",
    "           }\n",
    "    print data\n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "with open('repo_master_data_json.txt','r') as f:\n",
    "    master_data = json.load(f)['data']\n",
    "    repo_concrete_data = {}\n",
    "    error_list = []\n",
    "    for repo in master_data:\n",
    "        username = repo['owner']['login']\n",
    "        reponame = repo['name']\n",
    "        try:\n",
    "            repo_concrete_data[repo['id']] = scrape_repo_info(username, reponame)\n",
    "        except:\n",
    "            error_list.append((repo['id'], username, reponame))\n",
    "    with open('repo_concrete_data_json.txt', 'w') as ff:\n",
    "        ff.write(json.dumps(repo_concrete_data, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "    print error_list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_list = [(5344375, u'textmate', u'textmate'), (58836534, u'parkjs814', u'AlgorithmVisualizer'), (1549138, u'CocoaLumberjack', u'CocoaLumberjack'), (15953199, u'HubSpot', u'youmightnotneedjquery'), (24817507, u'moklick', u'frontend-stuff'), (132321, u'facebookarchive', u'three20'), (2281775, u'marcuswestin', u'WebViewJavascriptBridge'), (41592744, u'AllThingsSmitty', u'css-protips'), (20799673, u'kesenhoo', u'android-training-course-in-chinese'), (14454268, u'briangonzalez', u'jquery.adaptive-backgrounds.js'), (50301368, u'p-e-w', u'maybe'), (5894096, u'mdo', u'code-guide'), (21109196, u'PureLayout', u'PureLayout'), (48175620, u'google', u'agera'), (16466596, u'michaelvillar', u'dynamics.js'), (11423758, u'mame', u'quine-relay'), (12977854, u'breach', u'breach_core'), (25212911, u'davidsonfellipe', u'awesome-wpo'), (3159966, u'ubuwaits', u'beautiful-web-type'), (31125362, u'bendc', u'frontend-guidelines'), (5094437, u'ducksboard', u'gridster.js'), (14454268, u'briangonzalez', u'jquery.adaptive-backgrounds.js'), (50301368, u'p-e-w', u'maybe'), (5894096, u'mdo', u'code-guide'), (20167283, u'sbstjn', u'timesheet.js'), (21109196, u'PureLayout', u'PureLayout'), (55026106, u'huluoyang', u'freecodecamp.cn'), (48175620, u'google', u'agera'), (16466596, u'michaelvillar', u'dynamics.js'), (11423758, u'mame', u'quine-relay'), (12977854, u'breach', u'breach_core'), (25212911, u'davidsonfellipe', u'awesome-wpo'), (63484632, u'facebookresearch', u'fastText'), (3605299, u'baconjs', u'bacon.js'), (51994692, u'geeeeeeeeek', u'electronic-wechat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "error_list2 = []\n",
    "repo_concrete_data = {}\n",
    "for ID, username,reponame in error_list:\n",
    "    try:\n",
    "        repo_concrete_data[ID] = scrape_repo_info(username, reponame)\n",
    "    except IndexError:\n",
    "        error_list2.append((ID, username, reponame))\n",
    "with open('repo_concrete_data_json2.txt', 'w') as ff:\n",
    "    ff.write(json.dumps(repo_concrete_data, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "error_list = error_list2\n",
    "print len(error_list)\n",
    "print error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unique_data = {}\n",
    "for ID, username, reponame in set(error_list):\n",
    "    print ID, username, reponame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combining the data to a giant json file\n",
    "\"\"\"\n",
    "import json\n",
    "all_data = {}\n",
    "with open('repo_master_data_json.txt', 'r') as master_file:\n",
    "    master_data = json.load(master_file)['data']\n",
    "    for repo in master_data:\n",
    "        data = {\n",
    "            'id' : repo['id'],\n",
    "            'name' : repo['name'],\n",
    "            'created_at' : repo['created_at'],\n",
    "            'description' : repo['description'],\n",
    "            'forks_count' : repo['forks_count'],\n",
    "            'has_downloads' : repo['has_downloads'],\n",
    "            'has_issues' : repo['has_issues'],\n",
    "            'has_pages' : repo['has_pages'],\n",
    "            'has_wiki' : repo['has_wiki'],\n",
    "            'has_homepage' : True if repo['homepage'] else False,\n",
    "            'primary_language' : repo['language'],\n",
    "            'open_issues_count' : repo['open_issues_count'],\n",
    "            'size' : repo['size'],\n",
    "            'stars_count' : repo['stargazers_count'],\n",
    "            'watchers_count' : repo['watchers_count']\n",
    "        }\n",
    "        all_data[repo['id']] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('repo_languages_json.txt', 'r') as f:\n",
    "    language_data = json.load(f)\n",
    "    for ID, repo in all_data.iteritems():\n",
    "        repo['languages'] = language_data[str(ID)]['languages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('repo_concrete_contributors_json.txt', 'r') as f:\n",
    "    contributors_data = json.load(f)\n",
    "    missing_list = []\n",
    "    for ID, repo in all_data.iteritems():\n",
    "        repo['contributors'] = contributors_data[str(ID)] if str(ID) in contributors_data else []\n",
    "        if str(ID) not in contributors_data: missing_list.append((str(ID), repo['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('repo_concrete_data_json.txt', 'r') as f:\n",
    "    other_data = json.load(f)\n",
    "    for ID, repo in all_data.iteritems():\n",
    "        if str(ID) in other_data:\n",
    "            repo['branches_count'] = other_data[str(ID)]['branches']\n",
    "            repo['commits_count'] = other_data[str(ID)]['commits']\n",
    "            repo['contributors_count'] = other_data[str(ID)]['contributors']\n",
    "            repo['projects_count'] = other_data[str(ID)]['projects']\n",
    "            repo['pull_requests_count'] = other_data[str(ID)]['pull_requests']\n",
    "            repo['release_count'] = other_data[str(ID)]['release']\n",
    "        else:\n",
    "            repo['branches_count'] = -1\n",
    "            repo['commits_count'] = -1\n",
    "            repo['contributors_count'] = -1\n",
    "            repo['projects_count'] = -1\n",
    "            repo['pull_requests_count'] = -1\n",
    "            repo['release_count'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('repo_issue_comments_json.txt', 'r') as f:\n",
    "    comments_data = json.load(f)\n",
    "    for ID, repo in all_data.iteritems():\n",
    "        if str(ID) in comments_data:\n",
    "            repo['issue_comments_count'] = comments_data[str(ID)]['comments']\n",
    "        else:\n",
    "            repo['issue_comments_count'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('repo_milestones_json.txt', 'r') as f:\n",
    "    milestones_data = json.load(f)\n",
    "    for ID, repo in all_data.iteritems():\n",
    "        if str(ID) in milestones_data:\n",
    "            repo['milestones_count'] = len(milestones_data[str(ID)]['milestones'])\n",
    "        else:\n",
    "            repo['milestones_count'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data.values()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_data.json', 'w') as ff:\n",
    "    ff.write(json.dumps(all_data, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_data_array.json', 'w') as ff:\n",
    "    ff.write(json.dumps(all_data.values(), sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**INSIGHTS WE PLAN ON DISPLAYING**\n",
    "   - What language the most popular repositories use\n",
    "   - Most used language for these repositories\n",
    "   - When languages die, the time stamp for repositories that were created that use specific languages\n",
    "   - Is there a correlation between stars and number of commits?\n",
    "   - Is there a correlation between stars and amount of contributors?\n",
    "   - Is there a correlation between contributors and amount of commits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs489]",
   "language": "python",
   "name": "conda-env-cs489-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
